import tensorflow as tf

import operations as ops
import modelreader as mdr
import helper as hp

class Executor():

    def __init__(self, operations, inputs, data, graph_generator):
        self.operations = operations
        self.inputs = inputs
        self.data = data
        self.graph_generator = graph_generator

        self.first_dense = True
        self.meta = {}

        layers = []
        layers.append(self.inputs.to_string())
        for i in self.operations:
            layers.append(i.to_string())
        self.meta["layers"] = layers


    def get_meta(self):
        return self.meta
        

    def execute(self):
        
        with tf.Graph().as_default():

            with tf.Session() as sess:

                inputs = self.inputs.gen_op(self.data)

                for i in self.operations:

                    if (i.type() == ops.DENSE and self.first_dense):
                        self.first_dense = False
                        width = output.shape[1]
                        height = output.shape[2]
                        amount = output.shape[3]
                        flat = ops.Flat_OP(width, height, amount)
                        inputs = flat.gen_op(inputs)

                    inputs = i.gen_op(inputs)
                    init = tf.global_variables_initializer()
                    sess.run(init)
                    output = sess.run(inputs)

                detected_class = tf.argmax(input=inputs, axis=1)
                probabilities = tf.nn.softmax(inputs)
                detected_class_out = sess.run(detected_class)
                probabilities_out = sess.run(probabilities)

                self.meta["output"] = detected_class_out[0]
                probs = {}
                for index, num in enumerate(probabilities_out[0]):
                    probs[index] = float(num * 100)
                self.meta["probabilities"] = probs


if __name__ == "__main__":

    modelreader = mdr.ModelReader(hp.get_mnist_filename())
    data = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023529414, 0.76470596, 0.9960785, 1.0, 0.93725497, 0.1137255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023529414, 0.6392157, 0.9960785, 0.9960785, 0.6862745, 0.21568629, 0.011764707, 0.0, 0.0, 0.0, 0.0, 0.09019608, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47450984, 0.9960785, 0.9960785, 0.5803922, 0.03137255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.54901963, 0.9294118, 0.43921572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16862746, 0.9607844, 0.9960785, 0.9490197, 0.015686275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20784315, 0.92549026, 0.9960785, 0.43921572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6431373, 0.9960785, 0.9803922, 0.2509804, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6627451, 0.9960785, 0.97647065, 0.1254902, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.83921576, 0.9960785, 0.69803923, 0.0, 0.0, 0.011764707, 0.03529412, 0.03529412, 0.03529412, 0.17254902, 0.909804, 0.9960785, 0.64705884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.83921576, 0.9960785, 0.7254902, 0.0, 0.21176472, 0.7294118, 0.9960785, 0.9960785, 0.9960785, 0.9960785, 0.9960785, 0.9058824, 0.16862746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37647063, 0.9960785, 0.9803922, 0.74509805, 0.98823535, 0.9960785, 0.9960785, 0.9960785, 0.9960785, 0.9960785, 0.9960785, 0.50980395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49411768, 0.9960785, 0.9960785, 0.9960785, 0.9960785, 0.7803922, 0.46274513, 0.5686275, 0.9960785, 0.9960785, 0.9960785, 0.30588236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10980393, 0.47058827, 0.47058827, 0.47058827, 0.1254902, 0.011764707, 0.0, 0.47450984, 0.9960785, 0.9960785, 0.76470596, 0.015686275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43137258, 0.9960785, 0.9960785, 0.33333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41960788, 0.9960785, 0.98823535, 0.19215688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8235295, 0.9960785, 0.7176471, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.058823533, 0.87843144, 0.9960785, 0.2784314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27058825, 0.9960785, 0.97647065, 0.20784315, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8470589, 0.9960785, 0.92549026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8745099, 0.9960785, 0.74509805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8745099, 0.9960785, 0.73333335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8745099, 0.9960785, 0.97647065, 0.49411768, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44705886, 0.9333334, 0.9960785, 0.48627454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    inputs = ops.Input_OP(28, 28, 1)
    executor = Executor(modelreader.get_operations(), inputs, data, None)
    executor.execute()
    print(executor.get_meta())